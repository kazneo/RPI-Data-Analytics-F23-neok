{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "# import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torch import optim\n",
    "# from torch import nn\n",
    "# from torch import linalg as la\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# folder path to folder with Land Cover Type \n",
    "folder_path = '/data/flooding-prediction/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineElevLCTNoah(folder_path):\n",
    "    # a list of all CSV files in the folder\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    dataframes = []\n",
    "\n",
    "    for i in range(len(csv_files)):\n",
    "        print(\"Progress: \", str(round(i/len(csv_files)*100, 2)), \"%\")\n",
    "\n",
    "        file = csv_files[i]\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        dataframes.append(df)    \n",
    "\n",
    "    # Combine all dataframes for the current year\n",
    "    combined_df = pd.concat(dataframes)\n",
    "    print('CSV files are combined successfully!')\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = combineElevLCTNoah(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset from 2011 to 2016\n",
    "\n",
    "filtered_output = output[(output['DATE'] // 10000 >= 2011)]\n",
    "output2 = filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of null values by column\n",
    "\n",
    "null_counts = output2.isnull().sum()\n",
    "nulls = pd.DataFrame({'Null_Count': null_counts})\n",
    "\n",
    "# Display the result\n",
    "nulls.loc[nulls['Null_Count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values with 0\n",
    "\n",
    "output_updated = output2.drop(columns = ['TSUN_RISKS'])\n",
    "output_updated.replace(-9999, 0, inplace=True)\n",
    "output_updated = output_updated.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of null values after the replacement\n",
    "\n",
    "null_counts = output_updated.isnull().sum()\n",
    "nulls = pd.DataFrame({'Null_Count': null_counts})\n",
    "\n",
    "# Display the result\n",
    "nulls.loc[nulls['Null_Count'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for analyzing correlations\n",
    "selected_columns = ['CFLD_RISKS','HRCN_RISKS','ISTM_RISKS','FLD_ELEV', 'Elevation', 'Avg_Elevation_250m', 'Avg_Elevation_500m', 'Avg_Elevation_750m', 'Avg_Elevation_1000m', 'Avg_Elevation_1250m', 'Avg_Elevation_1500m', 'LC_Type1', 'LC_Type2', 'LC_Type3', 'LC_Type4', 'LC_Type5', 'LC_Prop1_Assessment', 'LC_Prop2_Assessment', 'LC_Prop3_Assessment', 'LC_Prop1', 'LC_Prop2', 'LC_Prop3', 'LW', 'QC', 'Qs', 'Qsm', 'Qsb', 'Rainf', 'Rainf_f', 'Snowf', 'SnowFrac', 'SnowDepth', 'SWE', 'Evap', 'Wind_f', 'SoilMoist0_10cm', 'SoilMoist10_40cm', 'SoilMoist40_100cm', 'SoilMoist100_200cm', 'SoilTemp0_10cm', 'SoilTemp10_40cm', 'SoilTemp40_100cm', 'SoilTemp100_200cm', 'ESoil', 'Streamflow', 'Tair_f', 'Tair_f_max', 'Tair_f_min', 'Qair_f', 'Psurf_f', 'STCOFIPS', 'TRACTFIPS', 'RFLD_RISKS', 'TRND_RISKS', 'FloodedFrac']\n",
    "selected_columns_df = output_updated[selected_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation = selected_columns_df.corr()\n",
    "\n",
    "# Setting for figure\n",
    "plt.figure(figsize=(100, 100))\n",
    "sns.set(font_scale=1.2)  \n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for analyzing correlations\n",
    "selected_columns = ['CFLD_RISKS','HRCN_RISKS','ISTM_RISKS','Elevation', 'LC_Type1', 'LC_Prop1_Assessment', 'LC_Prop1',  'QC', 'Rainf',  'Qsb',  'SoilMoist0_10cm','Streamflow','STCOFIPS', 'RFLD_RISKS', 'TRND_RISKS', 'FloodedFrac']\n",
    "selected_columns_df = output_updated[selected_columns]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation = selected_columns_df.corr()\n",
    "\n",
    "# Setting for figure\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.set(font_scale=1.2) \n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
